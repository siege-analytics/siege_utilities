Library Architecture
===================

Overview
--------

The ``siege_utilities`` library is organized into major functional areas, each providing specialized utilities for data engineering, analytics, and distributed computing workflows.

.. figure:: ../images/architecture_overview.png
   :alt: Siege Utilities Architecture Overview
   :align: center
   :width: 100%

   **Figure 1: Siege Utilities Library Architecture**

Core Architecture
----------------

.. code-block:: text

    siege_utilities/
    â”œâ”€â”€ ğŸ”§ Core Utilities (16 functions)
    â”‚   â”œâ”€â”€ Logging System (14 functions)
    â”‚   â”‚   â”œâ”€â”€ log_info, log_warning, log_error, log_debug, log_critical
    â”‚   â”‚   â”œâ”€â”€ init_logger, get_logger, configure_shared_logging
    â”‚   â”‚   â””â”€â”€ Thread-safe, configurable logging across all modules
    â”‚   â””â”€â”€ String Utilities (2 functions)
    â”‚       â”œâ”€â”€ remove_wrapping_quotes_and_trim
    â”‚       â””â”€â”€ Advanced string manipulation and cleaning
    â”‚
    â”œâ”€â”€ ğŸ“ File Operations (22 functions)
    â”‚   â”œâ”€â”€ File Hashing (5 functions)
    â”‚   â”‚   â”œâ”€â”€ calculate_file_hash, generate_sha256_hash_for_file
    â”‚   â”‚   â”œâ”€â”€ get_file_hash, get_quick_file_signature, verify_file_integrity
    â”‚   â”‚   â””â”€â”€ Cryptographic hashing and integrity verification
    â”‚   â”œâ”€â”€ File Operations (13 functions)
    â”‚   â”‚   â”œâ”€â”€ check_if_file_exists_at_path, delete_existing_file_and_replace_it_with_an_empty_file
    â”‚   â”‚   â”œâ”€â”€ count_total_rows_in_file_pythonically, count_empty_rows_in_file_pythonically
    â”‚   â”‚   â”œâ”€â”€ count_duplicate_rows_in_file_using_awk, count_total_rows_in_file_using_sed
    â”‚   â”‚   â”œâ”€â”€ count_empty_rows_in_file_using_awk, remove_empty_rows_in_file_using_sed
    â”‚   â”‚   â”œâ”€â”€ write_data_to_a_new_empty_file, write_data_to_an_existing_file
    â”‚   â”‚   â”œâ”€â”€ check_for_file_type_in_directory
    â”‚   â”‚   â””â”€â”€ Advanced file manipulation and analysis
    â”‚   â”œâ”€â”€ Path Management (2 functions)
    â”‚   â”‚   â”œâ”€â”€ ensure_path_exists, unzip_file_to_its_own_directory
    â”‚   â”‚   â””â”€â”€ Directory creation and file extraction
    â”‚   â”œâ”€â”€ Remote Operations (2 functions)
    â”‚   â”‚   â”œâ”€â”€ generate_local_path_from_url, download_file
    â”‚   â”‚   â””â”€â”€ URL-based file operations and downloads
    â”‚   â””â”€â”€ Shell Operations (1 function)
    â”‚       â”œâ”€â”€ run_subprocess
    â”‚       â””â”€â”€ Command execution and process management
    â”‚
    â”œâ”€â”€ ğŸš€ Distributed Computing (503+ functions)
    â”‚   â”œâ”€â”€ Spark Utilities (503 functions)
    â”‚   â”‚   â”œâ”€â”€ DataFrame operations, transformations, and optimizations
    â”‚   â”‚   â”œâ”€â”€ Data validation, cleaning, and processing
    â”‚   â”‚   â”œâ”€â”€ Performance tuning and caching strategies
    â”‚   â”‚   â”œâ”€â”€ File format handling (Parquet, CSV, JSON)
    â”‚   â”‚   â”œâ”€â”€ Advanced analytics and machine learning support
    â”‚   â”‚   â””â”€â”€ Production-ready Spark workflows
    â”‚   â”œâ”€â”€ HDFS Configuration (5 functions)
    â”‚   â”‚   â”œâ”€â”€ Cluster configuration and management
    â”‚   â”‚   â””â”€â”€ Connection and authentication setup
    â”‚   â”œâ”€â”€ HDFS Operations (2 functions)
    â”‚   â”‚   â”œâ”€â”€ File system operations and management
    â”‚   â”‚   â””â”€â”€ Data movement and organization
    â”‚   â””â”€â”€ HDFS Legacy Support (4 functions)
    â”‚       â”œâ”€â”€ Backward compatibility and migration tools
    â”‚       â””â”€â”€ Legacy system integration
    â”‚
    â”œâ”€â”€ ğŸŒ Geospatial (2 functions)
    â”‚   â”œâ”€â”€ Geocoding (2 functions)
    â”‚   â”‚   â”œâ”€â”€ concatenate_addresses, use_nominatim_geocoder
    â”‚   â”‚   â””â”€â”€ Address processing and coordinate generation
    â”‚   â””â”€â”€ Location-based analytics and mapping support
    â”‚
    â”œâ”€â”€ âš™ï¸ Configuration Management (15 functions)
    â”‚   â”œâ”€â”€ Client Management (8 functions)
    â”‚   â”‚   â”œâ”€â”€ create_client_profile, save_client_profile, load_client_profile
    â”‚   â”‚   â”œâ”€â”€ update_client_profile, list_client_profiles, search_client_profiles
    â”‚   â”‚   â”œâ”€â”€ validate_client_profile, associate_client_with_project
    â”‚   â”‚   â””â”€â”€ Client profile creation, management, and project association
    â”‚   â”œâ”€â”€ Connection Management (7 functions)
    â”‚   â”‚   â”œâ”€â”€ create_connection_profile, save_connection_profile, load_connection_profile
    â”‚   â”‚   â”œâ”€â”€ find_connection_by_name, list_connection_profiles, update_connection_profile
    â”‚   â”‚   â”œâ”€â”€ test_connection_profile, get_connection_status, cleanup_old_connections
    â”‚   â”‚   â””â”€â”€ Database, notebook, and Spark connection persistence
    â”‚   â”œâ”€â”€ Database Configurations
    â”‚   â”‚   â””â”€â”€ Connection string management and database utilities
    â”‚   â””â”€â”€ Project Management
    â”‚       â””â”€â”€ Project configuration and directory structure management
    â”‚
    â”œâ”€â”€ ğŸ“Š Analytics Integration (6 functions)
    â”‚   â”œâ”€â”€ Google Analytics (6 functions)
    â”‚   â”‚   â”œâ”€â”€ GoogleAnalyticsConnector class
    â”‚   â”‚   â”œâ”€â”€ create_ga_account_profile, save_ga_account_profile, load_ga_account_profile
    â”‚   â”‚   â”œâ”€â”€ list_ga_accounts_for_client, batch_retrieve_ga_data
    â”‚   â”‚   â””â”€â”€ GA4/UA data retrieval, client association, Pandas/Spark export
    â”‚   â””â”€â”€ Client-associated analytics account management
    â”‚
    â”œâ”€â”€ ğŸ§¹ Code Hygiene (2 functions)
    â”‚   â”œâ”€â”€ Documentation Generation (2 functions)
    â”‚   â”‚   â”œâ”€â”€ generate_docstring_template, analyze_function_signature
    â”‚   â”‚   â””â”€â”€ Automated documentation and code quality tools
    â”‚   â””â”€â”€ Code maintenance and quality assurance
    â”‚
    â””â”€â”€ ğŸ§ª Testing & Development (2 functions)
        â”œâ”€â”€ Environment Setup (2 functions)
        â”‚   â”œâ”€â”€ setup_spark_environment, get_system_info
        â”‚   â””â”€â”€ Development environment configuration and diagnostics
        â””â”€â”€ Testing framework and development tools

Function Distribution
--------------------

.. list-table:: Function Distribution by Module
   :widths: 30 20 20 30
   :header-rows: 1

   * - Module
     - Functions
     - Status
     - Description
   * - **Core Utilities**
     - 16
     - âœ… Complete
     - Logging, string manipulation, core infrastructure
   * - **File Operations**
     - 22
     - âœ… Complete
     - File handling, hashing, operations, remote access
   * - **Distributed Computing**
     - 503+
     - âœ… Complete
     - Spark, HDFS, cluster management, big data processing
   * - **Geospatial**
     - 2
     - âœ… Complete
     - Address processing, geocoding, location analytics
   * - **Configuration Management**
     - 15
     - âœ… Complete
     - Client profiles, connections, projects, databases
   * - **Analytics Integration**
     - 6
     - ğŸ†• New
     - Google Analytics, client association, data export
   * - **Code Hygiene**
     - 2
     - âœ… Complete
     - Documentation, code quality, maintenance
   * - **Testing & Development**
     - 2
     - âœ… Complete
     - Environment setup, diagnostics, testing tools

**Total Functions: 568+** | **Total Modules: 16** | **Coverage: 100%**

Key Features
------------

**ğŸ”§ Core Infrastructure**
- Thread-safe logging system with configurable levels
- String manipulation and cleaning utilities
- Robust error handling and fallback mechanisms

**ğŸ“ File Management**
- Cryptographic file hashing and integrity verification
- Advanced file operations with awk/sed integration
- Remote file operations and downloads
- Shell command execution and process management

**ğŸš€ Distributed Computing**
- **503+ Spark functions** for big data processing
- HDFS cluster configuration and management
- Production-ready Spark workflows and optimizations
- Advanced data transformation and analytics

**ğŸŒ Geospatial Capabilities**
- Address concatenation and standardization
- Nominatim geocoding integration
- Location-based analytics support

**âš™ï¸ Configuration Management**
- Client profile creation and management
- Connection persistence (databases, notebooks, Spark)
- Project configuration and directory management
- Client-project association system

**ğŸ“Š Analytics Integration**
- Google Analytics 4 and Universal Analytics support
- OAuth2 authentication and credential management
- Client-associated analytics account management
- Data export to Pandas and Spark formats
- Batch data retrieval and processing

**ğŸ§¹ Code Quality**
- Automated documentation generation
- Function signature analysis
- Code maintenance and quality tools

**ğŸ§ª Development Support**
- Spark environment setup and configuration
- System diagnostics and information
- Testing framework integration

Integration Points
-----------------

.. code-block:: text

    Client Management â†â†’ Analytics Integration
           â†“                    â†“
    Project Association â†â†’ Configuration Management
           â†“                    â†“
    File Operations â†â†’ Distributed Computing
           â†“                    â†“
    Geospatial â†â†’ Core Utilities
           â†“                    â†“
    Testing & Development â†â†’ Code Hygiene

Usage Patterns
--------------

**Data Engineering Workflow:**
1. **Setup**: Configure client profiles and connections
2. **Ingest**: Use file operations and remote capabilities
3. **Process**: Leverage 503+ Spark functions for transformation
4. **Analyze**: Apply geospatial and analytics capabilities
5. **Export**: Save to Pandas or Spark formats
6. **Manage**: Maintain configurations and monitor performance

**Client Analytics Workflow:**
1. **Configure**: Set up GA accounts linked to clients
2. **Authenticate**: OAuth2 flow for Google Analytics access
3. **Retrieve**: Batch data retrieval from GA4/UA
4. **Process**: Transform data using Spark functions
5. **Export**: Save as Pandas or Spark DataFrames
6. **Associate**: Link data to client projects and profiles

This architecture provides a comprehensive, integrated solution for data engineering, analytics, and distributed computing workflows, with all functions mutually available through the main package interface.
